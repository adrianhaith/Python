{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "777b0110",
   "metadata": {},
   "source": [
    "+ Illustration of policy gradient learning algorithm.\n",
    "\n",
    "First, define the task, which we will define in the style of gymnasium environment (a widely used RL library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0289a578",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "\n",
    "# agent model - defines the policy and learning rules\n",
    "class PGLearner:\n",
    "    def __init__(self, init_mean=None, init_std=None, alpha=0.01, alpha_nu=0.01, alpha_phi=0.01, rwd_baseline_decay=0.99):\n",
    "        self.alpha = alpha\n",
    "        self.alpha_nu = alpha_nu\n",
    "        self.alpha_phi = alpha_phi\n",
    "\n",
    "        # Defaults\n",
    "        if init_mean is None:\n",
    "            init_mean = np.array([0.5, 0.5])\n",
    "        if init_std is None:\n",
    "            init_std = np.array([.1, .1]) \n",
    "\n",
    "        self.init_mean = np.array(init_mean)\n",
    "        self.init_std = np.array(init_std)\n",
    "\n",
    "        # Learnable parameters (in normalized space)\n",
    "        self.mu_norm = np.zeros(2)                      # normalized mean = 0\n",
    "        self.nu = np.zeros(2)                           # log-eigenvalues in normalized units\n",
    "        self.phi = 0.0\n",
    "        self.Q = self._rotation_matrix(self.phi)\n",
    "\n",
    "        self.rwd_baseline = 0.0\n",
    "        self.rwd_baseline_decay = rwd_baseline_decay\n",
    "\n",
    "    def _rotation_matrix(self, phi):\n",
    "        return np.array([\n",
    "            [np.cos(phi), -np.sin(phi)],\n",
    "            [np.sin(phi),  np.cos(phi)]\n",
    "        ])\n",
    "\n",
    "    def _covariance_norm(self):\n",
    "        Lambda = np.diag(np.exp(self.nu))\n",
    "        return self.Q @ Lambda @ self.Q.T\n",
    "\n",
    "    def _to_normalized(self, action_real):\n",
    "        return (action_real - self.init_mean) / self.init_std\n",
    "\n",
    "    def _from_normalized(self, action_norm):\n",
    "        return self.init_mean + action_norm * self.init_std\n",
    "\n",
    "    def initialize_rwd_baseline(self, env, n_samples=100):\n",
    "        rewards = []\n",
    "        for _ in range(n_samples):\n",
    "            env.reset()\n",
    "            action = self.select_action()\n",
    "            _, reward, _, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "        self.rwd_baseline = np.mean(rewards)\n",
    "    \n",
    "    def select_action(self):\n",
    "        cov = self._covariance_norm()\n",
    "        a_norm = np.random.multivariate_normal(self.mu_norm, cov)\n",
    "        self._last_action_norm = a_norm\n",
    "        a_real = self._from_normalized(a_norm)\n",
    "        return a_real  # convert angle to radians for env\n",
    "\n",
    "    def update(self, action_real, reward):\n",
    "        # Convert back to real degrees for consistency with init_mean/init_std\n",
    "        #action_deg = np.array([np.rad2deg(action_real_rad[0]), action_real_rad[1]])\n",
    "        delta_real = action_real - self.init_mean\n",
    "        delta_norm = delta_real / self.init_std\n",
    "\n",
    "        # --- Mean update (in normalized space) ---\n",
    "        cov = self._covariance_norm()\n",
    "        grad_logp_mu = np.linalg.inv(cov) @ (delta_norm - self.mu_norm)\n",
    "        self.mu_norm += self.alpha * (reward - self.rwd_baseline) * grad_logp_mu\n",
    "\n",
    "        # --- ν update ---\n",
    "        z = self.Q.T @ (delta_norm - self.mu_norm)\n",
    "        lambdas = np.exp(self.nu)\n",
    "        grad_logp_nu = 0.5 * (-1 + (z ** 2) / lambdas)\n",
    "        self.nu += self.alpha_nu * (reward - self.rwd_baseline) * grad_logp_nu\n",
    "\n",
    "        # --- φ update ---\n",
    "        Lambda_inv = np.diag(1.0 / lambdas)\n",
    "        grad_logp_dQ = - self.Q.T @ np.outer(z, z) @ Lambda_inv\n",
    "        dQ_dphi = np.array([\n",
    "            [-np.sin(self.phi), -np.cos(self.phi)],\n",
    "            [ np.cos(self.phi), -np.sin(self.phi)]\n",
    "        ])\n",
    "        grad_logp_phi = np.trace(grad_logp_dQ.T @ dQ_dphi)\n",
    "        self.phi += self.alpha_phi * (reward - self.rwd_baseline) * grad_logp_phi\n",
    "        self.Q = self._rotation_matrix(self.phi)\n",
    "\n",
    "        # --- Update the reward baseline\n",
    "        self.rwd_baseline = self.rwd_baseline_decay * self.rwd_baseline + (1 - self.rwd_baseline_decay) * reward\n",
    "\n",
    "    # attribute to access the un-normalized mean\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self._from_normalized(self.mu_norm)\n",
    "    \n",
    "    # attribute to access the un-normalized covariance matrix\n",
    "    @property\n",
    "    def cov(self):\n",
    "        covariance_norm = self._covariance_norm()\n",
    "        scaling = np.array([[self.init_std[0],0], [0, self.init_std[1]]])\n",
    "        return scaling * covariance_norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
